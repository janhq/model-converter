name: GGUF Converter

on:
  workflow_dispatch:
    inputs:
      hf_model_id:
        description: "HuggingFace model ID"
        required: true
        type: string

env:
  USER_NAME: janhq
  MODEL_ID: ${{ inputs.hf_model_id }}
  QUANTIZATION_METHODS: "Q3_K_M Q4_K_M Q5_K_M Q8_0"

jobs:
  converter:
    runs-on: linux-cpu
    steps:
      - name: Clone
        id: checkout
        uses: actions/checkout@v3
        with:
          submodules: recursive

      - uses: actions/setup-python@v4
        with:
          python-version: "3.9"
          cache: "pip"

      - name: Login to HuggingFace Hub
        shell: bash
        run: |
          pip install huggingface_hub hf-transfer
          huggingface-cli login --token ${{ secrets.HUGGINGFACE_TOKEN }} --add-to-git-credential

      - name: Misc. env vars
        shell: bash
        run: |
          echo "Model ID: ${{ env.MODEL_ID }}"
          echo "Quantization: ${{ env.QUANTIZATION_METHODS }}"

          MODEL_ID=${{ env.MODEL_ID }}
          IFS='/' read -ra ADDR <<< "$MODEL_ID"
          ADDR_LENGTH=${#ADDR[@]}

          MODEL_NAME="${ADDR[$ADDR_LENGTH-1]}"
          if [[ "$MODEL_NAME" != "${MODEL_NAME,,}" ]]; then
            lowercase_model_name=$(echo "$MODEL_NAME" | tr '[:upper:]' '[:lower:]')
          else
            lowercase_model_name="$MODEL_NAME"
          fi
          echo "MODEL_NAME=$lowercase_model_name" >> $GITHUB_ENV

          fp16="${lowercase_model_name}.fp16.bin"
          echo "fp16=$fp16" >> $GITHUB_ENV

          repo_id="${{ env.USER_NAME }}/$lowercase_model_name"
          echo "repo_id=$repo_id" >> $GITHUB_ENV

      - name: Install llama.cpp dependencies
        shell: bash
        run: |
          cd llama.cpp/ 
          make clean
          make
          pip install -r requirements.txt

      - name: Download HF model
        shell: bash
        run: |
          HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download --repo-type model --local-dir ./models ${{ env.MODEL_ID }}

      - name: Convert to fp16
        shell: bash
        run: |
          mkdir "${{ env.MODEL_NAME }}"
          python llama.cpp/convert.py models --outtype f16 --outfile "${{ env.MODEL_NAME }}/${{ env.fp16 }}"

      - name: GGUF quantization and test
        shell: bash
        run: |
          for method in ${{ env.QUANTIZATION_METHODS }}; do
              # TODO: Add resources grab from logs
              qtype="${{ env.MODEL_NAME }}.$method.gguf"
              ./llama.cpp/quantize "${{ env.MODEL_NAME }}/${{ env.fp16 }}" "${{ env.MODEL_NAME }}/$qtype" "$method"

              ./llama.cpp/main -m "${{ env.MODEL_NAME }}/$qtype" -n 128 --color -ngl 35 -p "a"
          done

      - name: Upload to HF
        shell: bash
        run: |
          ls -la ./${{ env.MODEL_NAME }}
          huggingface-cli upload ${{ env.repo_id }} ${{ env.MODEL_NAME }} . --include *.gguf
          
      - name: Upload README to HF (TODO)
        shell: bash
        run: |
          ls -la ./${{ env.MODEL_NAME }}
          huggingface-cli upload ${{ env.repo_id }} ${{ env.MODEL_NAME }} . --include *.md


      - name: Cleanup
        if: always()
        shell: bash
        run: |
          rm -rf ./models
          rm -rf "${{ env.MODEL_NAME }}"
          rm -rf "/home/${USER}/.cache/huggingface/"
          huggingface-cli logout
